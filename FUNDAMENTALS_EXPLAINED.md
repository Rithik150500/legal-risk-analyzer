# Legal Risk Analyzer: Comprehensive Fundamentals Explained

## Table of Contents

1. [Introduction and Purpose](#introduction-and-purpose)
2. [Core Architecture Principles](#core-architecture-principles)
3. [The Multi-Agent System](#the-multi-agent-system)
4. [Data Room Indexing Pipeline](#data-room-indexing-pipeline)
5. [Tool Implementation Deep Dive](#tool-implementation-deep-dive)
6. [System Prompts and Agent Behavior](#system-prompts-and-agent-behavior)
7. [Human-in-the-Loop (HITL) System](#human-in-the-loop-hitl-system)
8. [Storage and State Management](#storage-and-state-management)
9. [Complete Workflow Walkthrough](#complete-workflow-walkthrough)
10. [Risk Categories and Analysis Methods](#risk-categories-and-analysis-methods)
11. [Technical Implementation Details](#technical-implementation-details)
12. [Best Practices and Deployment](#best-practices-and-deployment)
13. [Extension Points and Customization](#extension-points-and-customization)

---

## Introduction and Purpose

### What Problem Does This System Solve?

Legal document review is one of the most time-intensive and error-prone tasks in legal practice. When organizations need to analyze contracts, agreements, and legal documents for potential risks, traditional approaches require:

- **Hours of manual review** by expensive legal professionals
- **Risk of overlooking critical issues** buried in dense legal language
- **Inconsistent analysis** across different reviewers
- **No centralized view** of risk across multiple documents
- **Limited scalability** when dealing with large document collections

The Legal Risk Analyzer addresses these challenges by creating an automated, AI-powered system that can:

1. **Process diverse document types** (Word, Excel, PDF, PowerPoint) into a unified, searchable format
2. **Apply consistent risk analysis** across all documents using standardized criteria
3. **Generate professional deliverables** including executive reports and interactive dashboards
4. **Maintain human oversight** at critical decision points
5. **Scale efficiently** from a few documents to large due diligence exercises

### Core Philosophy

The system is built on several key principles:

**1. Augmentation, Not Replacement**: The system augments human expertise rather than replacing it. Legal professionals remain in control through strategic approval points, while AI handles the labor-intensive document review.

**2. Specialization Through Subagents**: Rather than having one monolithic agent try to do everything, the system uses specialized subagents. Each subagent is optimized for a specific task (analysis, reporting, visualization), leading to better results and efficient context management.

**3. Structured Data Flow**: Documents flow through a defined pipelineâ€”from raw files to indexed data room to analysis findings to final deliverables. This structured approach ensures consistency and traceability.

**4. Configurable Human Oversight**: Different use cases require different levels of human involvement. The system supports high oversight for critical analyses and minimal oversight for routine tasks.

---

## Core Architecture Principles

### Hierarchical Agent Design

The system uses a hierarchical multi-agent architecture where a main orchestrator agent coordinates specialized subagents:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Main Orchestrator     â”‚
                    â”‚   Agent                 â”‚
                    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
                    â”‚   â€¢ Plans analysis      â”‚
                    â”‚   â€¢ Coordinates work    â”‚
                    â”‚   â€¢ Synthesizes results â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                   â”‚                   â”‚
            â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Legal         â”‚   â”‚ Report        â”‚   â”‚ Dashboard     â”‚
    â”‚ Analyzer      â”‚   â”‚ Creator       â”‚   â”‚ Creator       â”‚
    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
    â”‚ â€¢ Doc review  â”‚   â”‚ â€¢ Synthesize  â”‚   â”‚ â€¢ Visualize   â”‚
    â”‚ â€¢ Risk assess â”‚   â”‚ â€¢ Format      â”‚   â”‚ â€¢ Interactive â”‚
    â”‚ â€¢ Web researchâ”‚   â”‚ â€¢ Word docs   â”‚   â”‚ â€¢ React UI    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Architecture?

**Context Window Management**: Large Language Models have limited context windows (the amount of text they can process at once). By using separate subagents, each can focus on its specific task without being overwhelmed by irrelevant information. The main agent keeps a high-level view (~10K tokens) while subagents handle detailed work (~50K tokens each).

**Specialized System Prompts**: Each agent receives a system prompt optimized for its role. The legal analyzer's prompt details risk categories and analysis methods. The report creator's prompt specifies document structure and formatting. This specialization produces better results than generic prompts.

**Parallel Execution**: Subagents can work concurrently on independent tasks. For example, the report creator and dashboard creator can work simultaneously once all analysis findings are available.

**Fault Isolation**: If a subagent encounters an error or produces poor results, the main agent can retry that specific task without losing work from other subagents.

### The DeepAgents Framework

This system is built on the `deepagents` framework, which provides:

- **Agent harness**: Infrastructure for creating and running agents
- **Subagent delegation**: The `task` tool for delegating work to specialized subagents
- **State management**: Checkpointing and persistence for long-running analyses
- **Tool integration**: Standard way to expose capabilities to agents
- **Backend routing**: Flexible storage backends for different data types

The framework integrates with LangGraph and LangChain for the underlying agent orchestration and LLM interaction.

---

## The Multi-Agent System

### Main Agent: The Orchestrator

**Location**: `legal_risk_analysis_agent.py` lines 437-507

The main agent serves as the coordinator for the entire analysis process. It does not perform detailed document analysis itselfâ€”instead, it:

1. **Reviews the data room index** to understand available documents
2. **Creates an analysis plan** using the `write_todos` tool
3. **Delegates analysis tasks** to the legal-analyzer subagent
4. **Collects findings** from all analyses
5. **Delegates deliverable creation** to report-creator and dashboard-creator subagents

**System Prompt Key Elements** (lines 165-203):

```python
MAIN_AGENT_SYSTEM_PROMPT = """You are a Legal Risk Analysis Deep Agent...

1. PLANNING: Create a comprehensive analysis plan
   - Review the data room index to understand available documents
   - Identify key legal areas that require investigation
   - Determine which documents need detailed review
   - Plan the sequence of analysis tasks

2. DELEGATING ANALYSIS TASKS: Use subagents effectively
   - Delegate document analysis to the "legal-analyzer" subagent
   - Each analysis task should focus on specific risk areas
   - Ensure comprehensive coverage of all legal risk dimensions

3. CREATING DELIVERABLES: Coordinate final outputs
   - Delegate report creation to "report-creator" subagent
   - Delegate dashboard creation to "dashboard-creator" subagent

WORKFLOW:
1. Use write_todos to create your analysis plan
2. Delegate analysis of different risk areas to "legal-analyzer" subagent
3. Synthesize findings from all analyses
4. Delegate report creation to "report-creator" subagent
5. Delegate dashboard creation to "dashboard-creator" subagent

Remember: You are the orchestrator. Your subagents do the detailed work while
you maintain the big picture and ensure comprehensive coverage."""
```

### Legal Analyzer Subagent

**Location**: `legal_risk_analysis_agent.py` lines 206-285

This subagent is the workhorse that performs actual document analysis. It follows a three-phase process:

**Phase 1: RETRIEVE**
- Access documents using `list_all_documents()`, `get_document()`, and `get_document_pages()`
- Conduct web research for legal context and precedents

**Phase 2: ANALYSE**
Examines documents across six detailed risk categories:

1. **Contractual Risks**: Ambiguous terms, unfavorable clauses, breach conditions
2. **Compliance Risks**: Regulatory violations, missing licenses
3. **IP Risks**: Ownership disputes, infringement issues
4. **Liability Risks**: Indemnification, warranties, limitations
5. **Financial Risks**: Payment terms, penalties, guarantees
6. **Operational Risks**: Obligations, deadlines, dependencies

**Phase 3: CREATE FINDINGS**
Writes structured findings to the filesystem:
- `/analysis/[risk_area]_findings.txt`
- Includes: Risk description, severity rating, document references, impact assessment, recommendations

**Why This Process?**

The retrieve-analyze-create pattern ensures:
- **Systematic coverage**: Every document is retrieved and analyzed against all applicable criteria
- **Evidence-based assessment**: Findings cite specific documents and clauses
- **Actionable output**: Each risk includes recommendations for mitigation
- **Context preservation**: Findings are saved to filesystem so the main agent's context isn't overwhelmed

### Report Creator Subagent

**Location**: `legal_risk_analysis_agent.py` lines 288-351

This subagent specializes in synthesizing analysis findings into professional Word documents.

**Input**: Reads all findings from `/analysis/` directory

**Output**: Creates `/outputs/legal_risk_analysis_report.docx`

**Report Structure**:

1. **Executive Summary** (1-2 pages)
   - Overall risk assessment
   - Key findings and critical risks
   - Summary of recommendations
   - Risk heat map

2. **Methodology**
   - Documents reviewed
   - Analysis framework
   - Risk classification criteria

3. **Detailed Risk Analysis** (by category)
   - Each risk with severity, documents affected, impact, mitigation

4. **Document-by-Document Analysis**
   - Summary of each document
   - Issues identified
   - Cross-references

5. **Recommendations**
   - Priority actions
   - Suggested amendments
   - Ongoing monitoring

6. **Appendices**
   - Definitions
   - References

### Dashboard Creator Subagent

**Location**: `legal_risk_analysis_agent.py` lines 354-430

This subagent creates interactive web-based visualizations for exploring risk data.

**Input**: Reads all findings from `/analysis/` directory

**Output**: Creates `/outputs/legal_risk_dashboard.html`

**Dashboard Features**:

1. **Risk Overview**
   - Heat map (severity vs likelihood)
   - Distribution charts
   - Overall risk score

2. **Category Tabs**
   - Separate tab for each risk category
   - Filterable content

3. **Risk Cards**
   - Title, description, severity badge
   - Affected documents
   - Recommendations

4. **Document Explorer**
   - List of all documents
   - Findings per document
   - Risk links

5. **Interactive Filters**
   - By severity
   - By category
   - By document
   - Search

**Technical Stack**:
- React for interactivity
- Tailwind CSS for styling
- Recharts for data visualization
- Single-file output (no external dependencies)

---

## Data Room Indexing Pipeline

### Purpose

Before any AI agent can analyze documents, those documents must be converted into a structured format the agents can efficiently navigate. The data room indexing pipeline transforms a folder of diverse document types into a JSON index with AI-generated summaries.

### The DataRoomIndexer Class

**Location**: `data_room_indexer.py` lines 26-396

```python
class DataRoomIndexer:
    def __init__(
        self,
        input_folder: str,      # Folder with original documents
        output_folder: str,     # Where to save processed files
        summarization_model: str = "gpt-4o-mini",  # AI model for summaries
        dpi: int = 200          # Image quality for page extraction
    ):
```

### Pipeline Stages

**Stage 1: Document Discovery**

The indexer scans the input folder for supported file types:
- Word documents: `.docx`, `.doc`
- Excel spreadsheets: `.xlsx`, `.xls`
- PowerPoint presentations: `.pptx`, `.ppt`
- PDFs: `.pdf`
- Text files: `.txt`, `.rtf`
- OpenDocument: `.odt`

**Stage 2: PDF Conversion** (lines 64-136)

All documents are converted to PDF using LibreOffice in headless mode:

```python
def convert_to_pdf(self, file_path: Path) -> Path:
    # Already PDF? Just copy
    if file_path.suffix.lower() == '.pdf':
        shutil.copy2(file_path, output_path)
        return output_path

    # Convert using LibreOffice
    subprocess.run([
        'libreoffice',
        '--headless',
        '--convert-to', 'pdf',
        '--outdir', str(self.pdfs_folder),
        str(file_path)
    ], check=True)
```

**Why LibreOffice?**
- Open source and free
- Handles all common document formats
- Cross-platform (Windows, macOS, Linux)
- Preserves formatting reasonably well
- Can run in headless mode (no GUI needed)

**Stage 3: Page Extraction** (lines 138-171)

Each PDF is split into individual page images using `pdf2image`:

```python
def extract_pages_as_images(self, pdf_path: Path, doc_id: str) -> List[Path]:
    images = convert_from_path(
        str(pdf_path),
        dpi=self.dpi,  # 200 DPI balances quality and size
        fmt='png'
    )

    page_paths = []
    for i, image in enumerate(images, start=1):
        page_path = doc_pages_folder / f"page_{i:03d}.png"
        image.save(page_path, 'PNG')
        page_paths.append(page_path)

    return page_paths
```

**Why Images Instead of Text Extraction?**
- Preserves tables, charts, and visual formatting
- Handles scanned documents
- Multimodal AI models can analyze visual layout
- More accurate for complex document structures

**Stage 4: Page Summarization** (lines 178-232)

Each page image is sent to an AI model for summarization:

```python
def summarize_page_with_ai(self, image_path: Path, page_num: int) -> str:
    prompt = f"""Analyze this document page (page {page_num}) and provide a concise summary.

    Focus on:
    - Main topics or sections covered
    - Key information (dates, parties, amounts, obligations)
    - Document type indicators (contract clauses, financial data, etc.)
    - Any critical legal terms or conditions

    Provide a 1-2 sentence summary that captures the essential content."""
```

**Stage 5: Document Summarization** (lines 234-278)

After all pages are summarized, those summaries are combined to create a document-level summary:

```python
def summarize_document_with_ai(self, page_summaries: List[str]) -> str:
    combined_summaries = "\n\n".join([
        f"Page {i+1}: {summary}"
        for i, summary in enumerate(page_summaries)
    ])

    prompt = f"""Based on these page-by-page summaries, provide a comprehensive
    2-3 sentence summary of the entire document.

    Focus on:
    - Document type and purpose
    - Main parties involved
    - Key terms, obligations, or information
    - Overall significance"""
```

### Output: The Data Room Index

The final output is a JSON structure:

```json
{
  "metadata": {
    "total_documents": 5,
    "created_at": "2025-01-15T10:00:00",
    "model_used": "gpt-4o-mini"
  },
  "documents": [
    {
      "doc_id": "doc_001",
      "original_file": "/dataroom/master_service_agreement.docx",
      "pdf_file": "/dataroom/pdfs/master_service_agreement.pdf",
      "summdesc": "Master Service Agreement between TechCorp Inc. and DataServices LLC...",
      "pages": [
        {
          "page_num": 1,
          "summdesc": "Title page identifying parties...",
          "page_image": "/dataroom/pages/doc_001/page_001.png"
        },
        {
          "page_num": 2,
          "summdesc": "Definitions section establishing key terms...",
          "page_image": "/dataroom/pages/doc_001/page_002.png"
        }
      ]
    }
  ]
}
```

### Benefits of This Structure

1. **Hierarchical Navigation**: Agents can start with document summaries, then drill down to specific pages only when needed

2. **Context Efficiency**: Summaries are compact, allowing agents to understand many documents without exhausting context

3. **Visual Access**: Page images are available for detailed inspection when summaries aren't sufficient

4. **Traceability**: Every piece of information links back to specific pages in original documents

---

## Tool Implementation Deep Dive

### The DataRoom Class

**Location**: `legal_risk_analysis_agent.py` lines 24-98

The `DataRoom` class wraps the data room index and provides methods for accessing documents:

```python
class DataRoom:
    def __init__(self, data_room_index: Dict[str, Any]):
        self.data_room_index = data_room_index

    def get_document_index(self) -> List[Dict[str, str]]:
        """Returns simplified index with doc_id and summdesc only"""
        return [
            {"doc_id": doc["doc_id"], "summdesc": doc["summdesc"]}
            for doc in self.data_room_index["documents"]
        ]

    def get_document_pages_summary(self, doc_id: str) -> str:
        """Returns combined summary of all pages"""
        doc = self.get_document(doc_id)
        if not doc:
            return f"Error: Document {doc_id} not found"

        page_summaries = [
            f"Page {page['page_num']}: {page['summdesc']}"
            for page in doc["pages"]
        ]
        return "\n\n".join(page_summaries)

    def get_document_pages_images(self, doc_id: str, page_nums: List[int]) -> List[Dict]:
        """Returns page images for specified pages"""
        # Returns page_num, page_image path, and summdesc
```

### Tool Creation Function

**Location**: `legal_risk_analysis_agent.py` lines 105-158

Tools are created using the `@tool` decorator from LangChain:

```python
def create_data_room_tools(data_room: DataRoom):
    """Creates tools for accessing the data room"""

    @tool
    def get_document(doc_id: str) -> str:
        """
        Retrieve a document's complete page-by-page summary.

        Args:
            doc_id: The unique identifier for the document (e.g., "doc_001")

        Returns:
            Combined summary of all pages in the document, with each page's
            summary clearly labeled by page number.

        Use this when you need to understand the full content of a document
        without viewing the actual page images.
        """
        return data_room.get_document_pages_summary(doc_id)

    @tool
    def get_document_pages(doc_id: str, page_nums: List[int]) -> str:
        """
        Retrieve specific page images and summaries from a document.

        Args:
            doc_id: The unique identifier for the document
            page_nums: List of page numbers to retrieve (e.g., [1, 3, 5])

        Returns:
            JSON string containing page images and summaries for the requested pages.
        """
        results = data_room.get_document_pages_images(doc_id, page_nums)
        return json.dumps(results, indent=2)

    @tool
    def list_all_documents() -> str:
        """
        List all documents available in the data room with their summaries.

        Returns:
            JSON string containing all documents with their doc_id and summdesc.
        """
        index = data_room.get_document_index()
        return json.dumps(index, indent=2)

    return [get_document, get_document_pages, list_all_documents]
```

### Why These Specific Tools?

**`list_all_documents`**: Provides high-level overview for planning which documents to analyze

**`get_document`**: Returns page summaries without imagesâ€”efficient for understanding document content

**`get_document_pages`**: Returns actual page images for detailed inspection when summaries aren't enough (e.g., viewing specific clauses, signatures, tables)

### Tool Documentation Best Practices

Notice how each tool has comprehensive documentation:

1. **Clear description** of what the tool does
2. **Argument documentation** with types and examples
3. **Return value description** explaining the format
4. **Usage guidance** explaining when to use the tool

This documentation is crucial because it's included in the agent's context, helping the LLM understand how to use each tool correctly.

---

## System Prompts and Agent Behavior

### The Role of System Prompts

System prompts are the primary way to configure agent behavior. They define:
- The agent's role and responsibilities
- Available tools and how to use them
- Workflows and processes to follow
- Output formats and quality standards

### Main Agent System Prompt Analysis

**Location**: `legal_risk_analysis_agent.py` lines 165-203

Key components:

1. **Role Definition**:
   ```
   You are a Legal Risk Analysis Deep Agent specializing in
   comprehensive legal risk assessment.
   ```

2. **Responsibility Structure**:
   ```
   1. PLANNING: Create a comprehensive analysis plan
   2. DELEGATING ANALYSIS TASKS: Use subagents effectively
   3. CREATING DELIVERABLES: Coordinate final outputs
   ```

3. **Context Injection**:
   ```python
   AVAILABLE DATA ROOM DOCUMENTS:
   {data_room_index}
   ```
   The actual document list is injected when the agent is created.

4. **Domain Knowledge**:
   ```
   KEY LEGAL RISK AREAS TO ASSESS:
   - Contractual risks (breaches, ambiguous terms, unfavorable clauses)
   - Compliance risks (regulatory violations, license issues)
   ...
   ```

5. **Workflow Definition**:
   ```
   WORKFLOW:
   1. Use write_todos to create your analysis plan
   2. Delegate analysis of different risk areas to "legal-analyzer" subagent
   ...
   ```

6. **Role Clarification**:
   ```
   Remember: You are the orchestrator. Your subagents do the detailed work
   while you maintain the big picture and ensure comprehensive coverage.
   ```

### Legal Analyzer System Prompt Analysis

**Location**: `legal_risk_analysis_agent.py` lines 206-285

This is the most detailed system prompt because the legal analyzer does the most complex work.

**Structure**:

1. **Process Definition**:
   ```
   YOUR PROCESS: RETRIEVE -> ANALYSE -> CREATE FINDINGS
   ```

2. **Retrieval Instructions**:
   ```
   1. RETRIEVE:
      - Use list_all_documents() to see what's available
      - Use get_document(doc_id) to retrieve full page-by-page summaries
      - Use get_document_pages(doc_id, page_nums) when you need actual images
      - Use web_search and web_fetch for legal precedents
   ```

3. **Detailed Analysis Criteria**:
   Each risk category has specific items to check. For example:
   ```
   CONTRACTUAL RISKS:
   - Identify ambiguous or undefined terms
   - Flag unfavorable or one-sided clauses
   - Note missing standard protections
   - Assess breach conditions and remedies
   ```

4. **Output Format Specification**:
   ```
   3. CREATE FINDINGS:
      Write your findings to the filesystem:
      - /analysis/[risk_area]_findings.txt
      - Include: Risk description, severity (High/Medium/Low),
        affected documents, specific clauses, recommendations,
        and supporting research
   ```

5. **Tool Reminder**:
   ```
   TOOLS AVAILABLE:
   - list_all_documents(): Get overview of all documents
   - get_document(doc_id): Get page-by-page summary
   ...
   ```

6. **Quality Guidelines**:
   ```
   IMPORTANT:
   - Be thorough but concise in your findings
   - Always cite specific documents and page numbers
   - Use web research to validate concerns with precedents
   - Save findings to filesystem for the main agent to compile
   - Focus on actionable risks with clear mitigation strategies
   ```

### Report Creator System Prompt

**Location**: `legal_risk_analysis_agent.py` lines 288-351

Focuses on:
- Reading findings from filesystem
- Synthesizing into report structure
- Professional formatting requirements
- Specific sections to include

### Dashboard Creator System Prompt

**Location**: `legal_risk_analysis_agent.py` lines 354-430

Includes:
- Dashboard feature requirements
- Technical stack specification (React, Tailwind, Recharts)
- Color scheme definitions
- UX priorities

---

## Human-in-the-Loop (HITL) System

### Why Human Oversight?

In legal contexts, AI outputs must be validated by qualified professionals because:
- Legal advice has serious consequences if wrong
- AI can hallucinate or misinterpret complex legal concepts
- Organizational risk tolerance varies
- Regulatory compliance may require human review

### The ApprovalLevel Class

**Location**: `hitl_implementation.py` lines 23-94

Provides pre-configured approval levels:

```python
class ApprovalLevel:
    @staticmethod
    def high_oversight():
        """All operations require approval"""
        return {
            "write_todos": True,
            "task": True,
            "get_document": True,
            "get_document_pages": True,
            "write_file": True,
            "edit_file": True,
        }

    @staticmethod
    def moderate_oversight():
        """Planning, delegation, and outputs require approval"""
        return {
            "write_todos": True,
            "task": True,
            "get_document": False,  # Auto-allow
            "get_document_pages": False,  # Auto-allow
            "write_file": True,
            "edit_file": {"allowed_decisions": ["approve", "reject"]},
        }

    @staticmethod
    def minimal_oversight():
        """Only final outputs require approval"""
        return {
            "write_todos": False,
            "task": False,
            "get_document": False,
            "get_document_pages": False,
            "write_file": True,
            "edit_file": True,
        }
```

### Review Interface System

**Base Class**: `ReviewInterface` (lines 101-121)

```python
class ReviewInterface:
    def review_action(
        self,
        action_request: Dict[str, Any],  # What the agent wants to do
        review_config: Dict[str, Any],   # Config for this review
        context: Optional[Dict[str, Any]] = None  # Additional info
    ) -> Dict[str, Any]:
        """Returns decision: approve, edit, or reject"""
        raise NotImplementedError
```

### CLI Review Interface

**Location**: `hitl_implementation.py` lines 124-415

Provides terminal-based review with:

```
ğŸ” HUMAN REVIEW REQUIRED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Tool: write_todos

ğŸ“ Proposed Action:
{
  "todos": [
    {"task": "Analyze doc_001", "priority": "high"}
  ]
}

Options:
  [1] âœ… APPROVE - Execute as proposed
  [2] âœï¸  EDIT - Modify before execution
  [3] âŒ REJECT - Skip this action
```

**Edit Capabilities**:
- Add/remove todos from plans
- Change priorities
- Add context to task delegations
- Modify file paths
- Edit content with external editor

### Auto-Approve Interface

**Location**: `hitl_implementation.py` lines 418-434

For testing or low-risk operations:

```python
class AutoApproveInterface(ReviewInterface):
    def __init__(self, log_actions: bool = True):
        self.log_actions = log_actions

    def review_action(self, action_request, review_config, context=None):
        if self.log_actions:
            print(f"[AUTO-APPROVED] {action_request['name']}")
        return {"type": "approve"}
```

### Audit Logging

**Location**: `hitl_implementation.py` lines 441-500

All review decisions are logged for compliance:

```python
class AuditLogger:
    def log_review(self, action_request, decision, reviewer, thread_id, context=None):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "thread_id": thread_id,
            "reviewer": reviewer,
            "action_tool": action_request["name"],
            "action_args": action_request["args"],
            "decision_type": decision["type"],
            "edited_args": decision.get("edited_action", {}).get("args"),
            "context": context
        }

        with open(self.log_file, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
```

### Running Agent with HITL

**Location**: `hitl_implementation.py` lines 544-629

```python
def run_agent_with_hitl(agent_config, user_message, thread_id, max_iterations=50):
    agent = agent_config["agent"]
    review_interface = agent_config["review_interface"]

    config = {"configurable": {"thread_id": thread_id}}

    # Initial invocation
    result = agent.invoke({
        "messages": [{"role": "user", "content": user_message}]
    }, config=config)

    # Handle interrupts
    while result.get("__interrupt__") and iteration < max_iterations:
        interrupts = result["__interrupt__"][0].value
        action_requests = interrupts["action_requests"]

        decisions = []
        for action in action_requests:
            decision = review_interface.review_action(
                action_request=action,
                review_config=config_map[action["name"]],
                context={"thread_id": thread_id}
            )
            decisions.append(decision)

            # Log if enabled
            if audit_logger:
                audit_logger.log_review(...)

        # Resume with decisions
        result = agent.invoke(
            Command(resume={"decisions": decisions}),
            config=config
        )

    return result
```

### When to Use Each Approval Level

**High Oversight**:
- First deployment of the system
- High-stakes analyses (M&A due diligence)
- Training period for understanding agent behavior
- Regulatory environments requiring documented review

**Moderate Oversight**:
- Routine contract reviews
- Trusted agent with track record
- Balance between efficiency and control

**Minimal Oversight**:
- Preliminary screenings
- Large-volume document processing
- Time-sensitive situations
- Mature deployments with proven accuracy

---

## Storage and State Management

### Backend Architecture

**Location**: `legal_risk_analysis_agent.py` lines 454-462

The system uses a `CompositeBackend` to route different data types to appropriate storage:

```python
def create_backend(runtime):
    return CompositeBackend(
        default=StateBackend(runtime),  # Agent state
        routes={
            "/memories/": StoreBackend(runtime),   # Long-term memory
            "/analysis/": StoreBackend(runtime),   # Analysis findings
        }
    )
```

### Why Multiple Backends?

**StateBackend** (default): Manages agent conversation stateâ€”the messages back and forth between user and agent. This is checkpointed so conversations can be resumed.

**StoreBackend** (for `/analysis/`): Persists analysis findings created by subagents. These need to survive beyond a single agent invocation and be accessible to other subagents.

**StoreBackend** (for `/memories/`): Long-term memory that persists across conversation threads. Could store things like learned preferences or previous analysis patterns.

### Checkpointing

```python
from langgraph.checkpoint.memory import MemorySaver

agent = create_deep_agent(
    ...
    checkpointer=MemorySaver(),  # In-memory for development
)
```

Checkpointing enables:
- Resuming interrupted analyses
- Handling HITL interrupts gracefully
- Multi-turn conversations
- State recovery after errors

For production, you'd use a persistent checkpointer like PostgresSaver.

### Thread IDs

Every analysis run uses a thread ID:

```python
config = {"configurable": {"thread_id": "legal_analysis_001"}}
result = agent.invoke({"messages": [...]}, config=config)
```

Thread IDs enable:
- Multiple concurrent analyses
- Conversation history per analysis
- State isolation between different matters
- Audit trail organization

---

## Complete Workflow Walkthrough

Let's trace through a complete analysis from start to finish.

### Step 1: Data Room Preparation

```python
from data_room_indexer import DataRoomIndexer

indexer = DataRoomIndexer(
    input_folder="/documents/acme-acquisition",
    output_folder="/processed/acme",
    summarization_model="gpt-4o-mini",
    dpi=200
)

data_room_index = indexer.build_data_room_index()
```

**What happens**:
1. Scanner finds 5 documents (MSA, NDA, SOW, Insurance Cert, Amendment)
2. Each is converted to PDF via LibreOffice
3. PDFs are split into page images (e.g., MSA has 10 pages)
4. AI summarizes each page (50 total page summaries)
5. AI creates document-level summaries (5 summaries)
6. JSON index is saved to `/processed/acme/data_room_index.json`

### Step 2: Agent Creation

```python
from legal_risk_analysis_agent import create_legal_risk_analysis_agent

agent = create_legal_risk_analysis_agent(data_room_index)
```

**What happens**:
1. DataRoom wrapper is created around index
2. Data room tools are instantiated (get_document, get_document_pages, list_all_documents)
3. Document index is formatted into system prompts
4. Three subagents are configured with their specialized prompts
5. Backend storage is configured
6. Main agent is created with all components

### Step 3: Analysis Invocation

```python
config = {"configurable": {"thread_id": "acme_analysis_001"}}

result = agent.invoke({
    "messages": [{
        "role": "user",
        "content": """Conduct comprehensive legal risk analysis of all documents.
        Focus on contractual, IP, and liability risks.
        Create a report and dashboard when done."""
    }]
}, config=config)
```

### Step 4: Main Agent Planning

The main agent receives the request and:

1. **Reviews available documents** by reading the index embedded in its system prompt

2. **Creates analysis plan** using `write_todos`:
   ```python
   write_todos([
       {"task": "Analyze MSA for contractual risks", "priority": "high"},
       {"task": "Analyze MSA for IP risks", "priority": "high"},
       {"task": "Analyze MSA for liability risks", "priority": "high"},
       {"task": "Analyze NDA for confidentiality", "priority": "high"},
       {"task": "Analyze SOW for operational risks", "priority": "medium"},
       {"task": "Review Insurance Certificate", "priority": "medium"},
       {"task": "Analyze Amendment impact", "priority": "high"},
       {"task": "Create comprehensive report", "priority": "high"},
       {"task": "Create interactive dashboard", "priority": "high"}
   ])
   ```

3. **If HITL enabled**: System interrupts for human review of plan

### Step 5: Analysis Delegation

Main agent delegates first task:

```python
task(
    name="legal-analyzer",
    task="""Analyze doc_001 (Master Service Agreement) for contractual risks.

    Focus on:
    - Ambiguous or undefined terms
    - One-sided or unfavorable clauses
    - Missing standard protections
    - Breach conditions and remedies
    - Automatic renewal provisions
    - Termination rights

    Save findings to /analysis/doc_001_contractual_risks.txt

    Available documents: [list from index]"""
)
```

### Step 6: Legal Analyzer Execution

The legal-analyzer subagent:

1. **Retrieves document**:
   ```python
   get_document("doc_001")
   # Returns page-by-page summaries of MSA
   ```

2. **Analyzes content** against contractual risk criteria:
   - Finds automatic renewal clause (Page 9)
   - Notes one-sided termination rights (Page 9)
   - Identifies liability cap (Page 8)
   - Notes indemnification limitations (Page 7)

3. **Conducts web research**:
   ```python
   web_search("automatic renewal clause enforceability by state")
   ```

4. **Creates findings**:
   ```python
   write_file(
       "/analysis/doc_001_contractual_risks.txt",
       """
       CONTRACTUAL RISK ANALYSIS - doc_001 (Master Service Agreement)

       RISK #1: Automatic Renewal Clause
       Severity: HIGH
       Location: Page 9

       Description: Agreement automatically renews unless either party
       provides 60 days notice before term end. This creates risk of
       unintended contract extension.

       Impact: Could lock organization into unfavorable terms for
       additional 3-year period.

       Recommendation: Calendar reminder system for renewal notice
       deadline. Consider negotiating 90-day notice period.

       RISK #2: One-Sided Termination Rights
       ...
       """
   )
   ```

5. **If HITL enabled**: Interrupt for human review of findings before saving

### Step 7: Repeat for All Analyses

Main agent continues delegating tasks:
- MSA IP risks â†’ legal-analyzer â†’ findings saved
- MSA liability risks â†’ legal-analyzer â†’ findings saved
- NDA analysis â†’ legal-analyzer â†’ findings saved
- SOW analysis â†’ legal-analyzer â†’ findings saved
- Insurance review â†’ legal-analyzer â†’ findings saved
- Amendment impact â†’ legal-analyzer â†’ findings saved

### Step 8: Report Creation

Main agent delegates report creation:

```python
task(
    name="report-creator",
    task="""Create comprehensive Word document report from all analysis findings.

    Read all files from /analysis/ directory.

    Include:
    - Executive summary with overall risk assessment
    - Detailed findings by risk category
    - Document-by-document analysis
    - Prioritized recommendations

    Save to /outputs/legal_risk_analysis_report.docx"""
)
```

Report-creator subagent:
1. Reads all findings files
2. Synthesizes into report structure
3. Creates Word document with professional formatting
4. Saves to output location

### Step 9: Dashboard Creation

Main agent delegates dashboard creation:

```python
task(
    name="dashboard-creator",
    task="""Create interactive web dashboard for risk visualization.

    Read all files from /analysis/ directory.

    Include:
    - Risk overview with heat map and charts
    - Category tabs for each risk type
    - Detailed risk cards with severity badges
    - Document explorer with risk links
    - Interactive filters (severity, category, document)

    Use React with Tailwind CSS and Recharts.

    Save to /outputs/legal_risk_dashboard.html"""
)
```

Dashboard-creator subagent:
1. Reads and structures findings data
2. Creates React component with visualizations
3. Embeds all data in single HTML file
4. Saves to output location

### Step 10: Completion

Main agent confirms completion:

```
Analysis complete! I have:

1. Analyzed all 5 documents for contractual, IP, liability, compliance,
   financial, and operational risks

2. Created detailed findings for 23 identified risks:
   - 7 High severity
   - 11 Medium severity
   - 5 Low severity

3. Generated deliverables:
   - Report: /outputs/legal_risk_analysis_report.docx
   - Dashboard: /outputs/legal_risk_dashboard.html

Key high-priority findings:
- Automatic renewal clause in MSA needs immediate attention
- Amendment doc_005 significantly weakened Provider protections
- Insurance coverage may be insufficient for contract value

I recommend reviewing the executive summary first, then using the
dashboard to explore specific risks by category.
```

---

## Risk Categories and Analysis Methods

### The Seven Risk Categories

**1. Contractual Risks**

What to look for:
- Ambiguous or undefined terms that could be interpreted differently
- One-sided clauses favoring the counterparty
- Missing standard protections (e.g., no cure period for breaches)
- Automatic renewal without adequate notice period
- Change of control provisions
- Assignment restrictions

Severity assessment factors:
- Likelihood of occurrence
- Financial exposure if triggered
- Ability to mitigate or cure
- Precedent in similar situations

**2. Compliance Risks**

What to look for:
- Regulatory requirements specific to the transaction
- Required licenses or permits
- Data privacy obligations (GDPR, CCPA, HIPAA)
- Industry-specific regulations
- Export control requirements
- Anti-corruption provisions

Severity assessment factors:
- Regulatory penalties for non-compliance
- Reputational damage
- Business continuity impact
- Cost of compliance implementation

**3. Intellectual Property Risks**

What to look for:
- Unclear IP ownership provisions
- Work-for-hire vs. licensed IP
- Background IP vs. foreground IP
- Open source license obligations
- IP indemnification scope and limits
- Non-compete and non-solicit restrictions

Severity assessment factors:
- Value of IP involved
- Exclusivity of rights granted
- Duration of restrictions
- Geographic scope

**4. Liability Risks**

What to look for:
- Indemnification obligations (scope, caps, procedures)
- Warranty representations
- Limitation of liability clauses
- Insurance requirements
- Consequential damages exclusions
- Survival periods for obligations

Severity assessment factors:
- Cap amounts relative to contract value
- Carve-outs from caps
- Insurance coverage adequacy
- Historical claims experience

**5. Financial Risks**

What to look for:
- Payment terms and conditions
- Late payment penalties and interest
- Pricing adjustment mechanisms
- Currency and exchange rate provisions
- Financial guarantees or security
- Audit rights

Severity assessment factors:
- Cash flow impact
- Credit risk exposure
- Total financial commitment
- Exit costs

**6. Operational Risks**

What to look for:
- Performance obligations and deadlines
- Service levels and remedies
- Dependencies on third parties
- Force majeure provisions
- Change management procedures
- Transition and termination assistance

Severity assessment factors:
- Business criticality of deliverables
- Penalty/remedy amounts
- Alternative sources available
- Timeline constraints

**7. Reputational Risks**

What to look for:
- Confidentiality obligations
- Public announcement restrictions
- Reference rights
- Conflicts of interest
- Ethical conduct requirements
- ESG and sustainability provisions

Severity assessment factors:
- Potential for public disclosure
- Brand and stakeholder impact
- Regulatory notification requirements
- Social media amplification potential

### Severity Rating Framework

**High Severity**:
- Significant financial exposure (>10% of contract value)
- Critical operational impact
- Regulatory non-compliance penalties
- Irreversible consequences
- No mitigation available
- High likelihood of occurrence

**Medium Severity**:
- Moderate financial exposure (2-10% of contract value)
- Material operational impact
- Regulatory scrutiny possible
- Reversible with effort
- Mitigation available but complex
- Medium likelihood of occurrence

**Low Severity**:
- Minor financial exposure (<2% of contract value)
- Limited operational impact
- No regulatory implications
- Easily reversible
- Simple mitigation available
- Low likelihood of occurrence

---

## Technical Implementation Details

### Agent Creation Function

**Location**: `legal_risk_analysis_agent.py` lines 437-507

```python
def create_legal_risk_analysis_agent(data_room_index: Dict[str, Any]):
    # Initialize data room wrapper
    data_room = DataRoom(data_room_index)

    # Create tools that access the data room
    data_room_tools = create_data_room_tools(data_room)

    # Configure storage backend
    def create_backend(runtime):
        return CompositeBackend(
            default=StateBackend(runtime),
            routes={
                "/memories/": StoreBackend(runtime),
                "/analysis/": StoreBackend(runtime),
            }
        )

    # Format data for system prompts
    data_room_index_text = json.dumps(data_room.get_document_index(), indent=2)

    # Define subagents
    subagents = [
        {
            "name": "legal-analyzer",
            "description": "...",
            "system_prompt": ANALYSIS_SUBAGENT_SYSTEM_PROMPT.format(
                data_room_index=data_room_index_text
            ),
            "tools": data_room_tools,
            "model": "claude-sonnet-4-5-20250929",
        },
        {
            "name": "report-creator",
            "description": "...",
            "system_prompt": REPORT_CREATOR_SYSTEM_PROMPT,
            "tools": [],  # Uses filesystem tools from middleware
            "model": "claude-sonnet-4-5-20250929",
        },
        {
            "name": "dashboard-creator",
            "description": "...",
            "system_prompt": DASHBOARD_CREATOR_SYSTEM_PROMPT,
            "tools": [],
            "model": "claude-sonnet-4-5-20250929",
        },
    ]

    # Create main agent
    agent = create_deep_agent(
        model="claude-sonnet-4-5-20250929",
        system_prompt=MAIN_AGENT_SYSTEM_PROMPT.format(
            data_room_index=data_room_index_text
        ),
        tools=data_room_tools,
        subagents=subagents,
        backend=create_backend,
        store=InMemoryStore(),
        checkpointer=MemorySaver(),
    )

    return agent
```

### Model Selection

The system uses Claude claude-sonnet-4-5-20250929 throughout:

```python
"model": "claude-sonnet-4-5-20250929"
```

This choice provides:
- Strong legal document understanding
- Detailed analysis capabilities
- Good instruction following
- Reasonable cost/quality balance

For specific tasks, you might consider:
- More capable models (Opus) for complex legal reasoning
- Faster models (Haiku) for high-volume page summarization
- Alternative providers (GPT-4, Gemini) based on preference

### Dependencies

**Python Packages** (from requirements.txt):

```
deepagents>=0.1.0        # Agent framework
langgraph>=0.2.0         # Graph-based agents
langchain>=0.3.0         # LLM chains
langchain-anthropic>=0.3.0  # Claude integration
langchain-openai>=0.2.0  # OpenAI integration
pdf2image>=1.16.3        # PDF to image conversion
Pillow>=10.0.0           # Image processing
python-docx>=1.1.0       # Word document creation
openpyxl>=3.1.0          # Excel file handling
tavily-python>=0.3.0     # Web search
```

**System Dependencies**:

```bash
# Ubuntu/Debian
sudo apt-get install libreoffice poppler-utils

# macOS
brew install --cask libreoffice
brew install poppler

# Windows
# Download LibreOffice from libreoffice.org
# Install poppler via conda or pre-built binaries
```

---

## Best Practices and Deployment

### Development Best Practices

**1. Start with demo_quick_start.py**
```bash
python demo_quick_start.py
```
This validates your environment without making API calls.

**2. Use mock data first**
The `create_mock_data_room()` function in `example_usage.py` provides realistic test data.

**3. Test with high oversight**
During development, use `ApprovalLevel.high_oversight()` to see every decision the agent makes.

**4. Monitor token usage**
Large documents can consume significant context. Watch for:
- Agent responses getting cut off
- Loss of context in later parts of conversation
- Subagent findings growing too large

**5. Version your system prompts**
Track changes to prompts as you refine agent behavior.

### Production Deployment

**1. Use persistent storage**

Replace in-memory stores with databases:

```python
from langgraph.store.postgres import PostgresStore
from langgraph.checkpoint.postgres import PostgresSaver

store = PostgresStore(connection_string="postgresql://...")
checkpointer = PostgresSaver(connection_string="postgresql://...")
```

**2. Configure appropriate oversight**

Start with moderate oversight and adjust based on experience:

```python
approval_level = ApprovalLevel.moderate_oversight()
```

**3. Set up audit logging**

The `AuditLogger` writes to JSONL. In production, consider:
- Centralized logging (ELK stack, Datadog)
- Compliance-ready exports
- Retention policies

**4. Implement error handling**

Wrap agent invocations in try/catch:

```python
try:
    result = agent.invoke(...)
except Exception as e:
    logger.error(f"Agent error: {e}")
    # Notify team, save state, etc.
```

**5. Set up monitoring**

Track:
- API costs per analysis
- Time per analysis
- Approval rates and edit frequencies
- Error rates and types

### Security Considerations

**1. API Key Management**
- Use environment variables or secrets managers
- Never commit keys to code
- Rotate keys periodically

**2. Document Access Control**
- Validate doc_id inputs
- Implement document-level permissions
- Log all document access

**3. Output Validation**
- Review generated files before distribution
- Check for sensitive information leakage
- Validate file paths to prevent traversal

**4. Network Security**
- Use HTTPS for web research
- Consider proxy configuration
- Monitor external API calls

---

## Extension Points and Customization

### Adding New Risk Categories

1. **Update Legal Analyzer prompt** (`legal_risk_analysis_agent.py`):

```python
ANALYSIS_SUBAGENT_SYSTEM_PROMPT = """...

   ENVIRONMENTAL RISKS:
   - Compliance with environmental regulations
   - Remediation obligations
   - Carbon reporting requirements
   - Sustainability commitments

..."""
```

2. **Update Dashboard Creator prompt** to add new tab

3. **Retrain** team on new category criteria

### Adding New Subagents

```python
# In create_legal_risk_analysis_agent():

subagents = [
    # ... existing subagents ...
    {
        "name": "contract-comparator",
        "description": "Specialist in comparing current contract against standard templates or previous versions",
        "system_prompt": COMPARATOR_SYSTEM_PROMPT,
        "tools": comparison_tools,
        "model": "claude-sonnet-4-5-20250929",
    },
]
```

### Adding New Tools

```python
@tool
def get_similar_contracts(doc_id: str, contract_type: str) -> str:
    """
    Retrieve similar contracts from the organization's contract database.

    Args:
        doc_id: Current document being analyzed
        contract_type: Type of contract (MSA, NDA, SOW, etc.)

    Returns:
        JSON list of similar contracts with key terms for comparison
    """
    # Implementation to query contract database
    return json.dumps(similar_contracts)
```

### Custom Review Interfaces

```python
class SlackReviewInterface(ReviewInterface):
    def __init__(self, webhook_url, approval_channel):
        self.webhook_url = webhook_url
        self.approval_channel = approval_channel

    def review_action(self, action_request, review_config, context=None):
        # Post to Slack with action buttons
        # Wait for response via webhook
        # Return decision
        pass
```

### Integration with Document Management

```python
class NetDocumentsDataRoom(DataRoom):
    def __init__(self, api_client, cabinet_id):
        self.client = api_client
        self.cabinet_id = cabinet_id

    def get_document(self, doc_id):
        # Fetch from NetDocuments API
        doc = self.client.get_document(self.cabinet_id, doc_id)
        return self._convert_to_index_format(doc)
```

---

## Conclusion

The Legal Risk Analyzer is a sophisticated system that combines:

- **Multi-agent architecture** for specialization and context efficiency
- **Structured data pipeline** for consistent document processing
- **Comprehensive risk analysis** across seven major categories
- **Professional deliverables** in both static and interactive formats
- **Human oversight** at configurable approval points

Understanding these fundamentals enables you to:

1. **Deploy the system effectively** for your use cases
2. **Customize risk categories** for your industry
3. **Extend capabilities** with new subagents and tools
4. **Integrate with your existing** document management and workflow systems
5. **Maintain appropriate oversight** for regulatory compliance

The system represents a pattern for AI-assisted professional services that balances automation efficiency with human expertise and oversightâ€”a model applicable beyond legal analysis to any domain requiring careful document review and risk assessment.

---

## Quick Reference

### Common Operations

**Create and run basic analysis**:
```python
from legal_risk_analysis_agent import create_legal_risk_analysis_agent
from example_usage import create_mock_data_room

agent = create_legal_risk_analysis_agent(create_mock_data_room())
config = {"configurable": {"thread_id": "analysis_001"}}
result = agent.invoke({
    "messages": [{"role": "user", "content": "Analyze all documents"}]
}, config=config)
```

**Run with human oversight**:
```python
from hitl_implementation import (
    create_agent_with_hitl, run_agent_with_hitl,
    ApprovalLevel, CLIReviewInterface
)

agent_config = create_agent_with_hitl(
    data_room_index=data_room_index,
    approval_level=ApprovalLevel.moderate_oversight(),
    review_interface=CLIReviewInterface()
)

result = run_agent_with_hitl(
    agent_config=agent_config,
    user_message="Analyze contractual risks",
    thread_id="analysis_001"
)
```

**Preprocess documents**:
```python
from data_room_indexer import DataRoomIndexer

indexer = DataRoomIndexer(
    input_folder="/documents",
    output_folder="/processed",
    summarization_model="gpt-4o-mini",
    dpi=200
)
index = indexer.build_data_room_index()
```

### File Locations

| File | Purpose |
|------|---------|
| `legal_risk_analysis_agent.py` | Main agent system, tools, prompts |
| `data_room_indexer.py` | Document preprocessing pipeline |
| `hitl_implementation.py` | Human oversight workflows |
| `example_usage.py` | Usage patterns and examples |
| `demo_quick_start.py` | Environment validation |

### Key Classes and Functions

| Component | Location | Purpose |
|-----------|----------|---------|
| `DataRoom` | `legal_risk_analysis_agent.py:24` | Document access wrapper |
| `create_data_room_tools` | `legal_risk_analysis_agent.py:105` | Tool factory |
| `create_legal_risk_analysis_agent` | `legal_risk_analysis_agent.py:437` | Agent factory |
| `DataRoomIndexer` | `data_room_indexer.py:26` | Preprocessing pipeline |
| `ApprovalLevel` | `hitl_implementation.py:23` | HITL configurations |
| `CLIReviewInterface` | `hitl_implementation.py:124` | Terminal review UI |
| `AuditLogger` | `hitl_implementation.py:441` | Decision logging |
| `create_agent_with_hitl` | `hitl_implementation.py:507` | HITL agent factory |
| `run_agent_with_hitl` | `hitl_implementation.py:544` | HITL execution |

---

*This document provides a comprehensive explanation of the Legal Risk Analyzer system fundamentals. For specific implementation details, refer to the source code files. For deployment guidance, see README.md and SYSTEM_OVERVIEW.md.*
